#!/bin/sh

# Expect this script to run for 10+ hours and to consume 200+ GB of RAM.
#
# Usage:
#     ./process
# or, because this is a long running task
#     ./process &> progress &

cd $(dirname $0)

# Path to the Twitter dataset
SRC_DIR=../../data/Twitter/raw

# List of the tweet files to process.
SRC_TWEETS=$SRC_DIR/tweets2009-*.txt

# Map between numeric identifiers and screen names.
SRC_USERMAP=$SRC_DIR/numeric2screen

# Links between users.
SRC_GRAPH=$SRC_DIR/twitter_rv.net

# Intermediate output
INT_DATA=/tmp/twitter.data
INT_GRAPH=/tmp/twitter.graph

# Output *.data and *.graph files.
OUT_DATA=../../data/Twitter/twitter.data
OUT_GRAPH=../../data/Twitter/twitter.graph
OUT_KEYMAP=../../data/Twitter/twitter.keymap


# Process the tweets to create the data.
if ! [ -f $INT_DATA ]; then
    ./data --usermap $SRC_USERMAP $SRC_TWEETS > $INT_DATA
fi

# Process the graph and prune dead edges.
if ! [ -f $INT_GRAPH ]; then
    ./graph $SRC_GRAPH | ../../tools/prune_edges.py $INT_DATA > $INT_GRAPH
fi

# Normalize node indices
../../tools/normalize.py $INT_DATA $INT_GRAPH $OUT_DATA $OUT_GRAPH $OUT_KEYMAP
